Fixed Version (Replace the JSON reading and extraction part)
Why This Works Better

Reads the entire JSON file as a string → no schema mismatch
Uses from_json with MapType(String, String) → treats everything as key-value, very forgiving
Allows extra fields in nested objects without failing
Still supports multiline JSON
Keeps watermark folder skipping intact

Replace Only This Part
Replace everything from df_raw = spark.read... down to the df_final creation with the code block above.
# --- RELIABLE METHOD: Read as wholeTextFiles (string) ---
            from pyspark.sql.functions import input_file_name

            # Read all JSON files as text (one row per file)
            df_text = spark.read.text(f"{shortcut_folder}/**/*.json")

            if df_text.rdd.isEmpty():
                logger.info(f"No JSON files found under '{shortcut_name}'")
                continue

            # Add file path and shortcut name
            df_text = df_text.withColumn("file_path", input_file_name())
            df_text = df_text.withColumn("shortcut_name", lit(shortcut_name))

            # Filter out files in watermark folders
            df_text = df_text.withColumn("path_parts", split(col("file_path"), "/"))
            df_text = df_text.withColumn("lower_parts", transform(col("path_parts"), lambda x: lower(x)))
            df_text = df_text.withColumn("has_watermark", array_contains(col("lower_parts"), "watermark"))
            df_text = df_text.filter(~col("has_watermark"))

            if df_text.rdd.isEmpty():
                logger.info(f"All JSON files in '{shortcut_name}' are in watermark folder → skipped")
                continue

            # Now parse the JSON string safely using from_json with MapType
            from pyspark.sql.types import MapType, StringType
            from pyspark.sql.functions import from_json

            json_options = {"multiLine": True}
            df_parsed = df_text.withColumn(
                "json_map",
                from_json(col("value"), MapType(StringType(), StringType()), json_options)
            )

            # Extract top-level fields
            df_extracted = df_parsed.select(
                "shortcut_name",
                col("json_map.datasetName").alias("dataset_name"),
                col("json_map.datasetTypeName").alias("dataset_type_name"),
                col("json_map.databaseName").alias("database_name")
            )

            # Extract nested fields using getItem on deeper maps
            df_extracted = df_extracted.withColumn(
                "source_props",
                col("json_map.sourceSystemProperties")
            ).withColumn(
                "curated_props",
                col("json_map.curatedProperties")
            )

            # Now safely extract nested values (returns null if missing)
            df_extracted = df_extracted.withColumn(
                "source_system_ingest_type",
                col("source_props.ingestType")
            ).withColumn(
                "target_load_type",
                col("curated_props.targetLoadType")
            )

            # Final select
            df_final = df_extracted.select(
                "shortcut_name",
                "dataset_name",
                "dataset_type_name",
                "database_name",
                "source_system_ingest_type",
                "target_load_type"
            )

            all_data_frames.append(df_final)
