def run_load_datasets_metadata():
    ensure_target_table()

    logger.info(f"Loading shortcuts config from {CONFIG_PATH}")
    config = load_shortcuts_config(CONFIG_PATH)
    shortcuts = config.get("shortcuts", [])

    if not shortcuts:
        logger.info("No shortcuts defined in config.")
        return

    all_data_frames = []

    # Import required functions
    from pyspark.sql.functions import (
        input_file_name, split, transform, lower as spark_lower,
        array_contains, lit, from_json, col
    )
    from pyspark.sql.types import MapType, StringType

    for entry in shortcuts:
        shortcut_name = entry.get("name")
        if not shortcut_name:
            logger.warning("Skipping entry without 'name'")
            continue

        shortcut_folder = f"Files/{shortcut_name}"
        json_pattern = f"{shortcut_folder}/**/*.json"

        logger.info(f"Processing shortcut '{shortcut_name}' → {json_pattern}")

        try:
            # Read each JSON file as whole text
            df_text = spark.read.text(json_pattern)

            if df_text.rdd.isEmpty():
                logger.info(f"No JSON files found under '{shortcut_name}'")
                continue

            df_text = df_text.withColumn("file_path", input_file_name())
            df_text = df_text.withColumn("shortcut_name", lit(shortcut_name))

            # === Skip files inside any "watermark" folder (case-insensitive) ===
            df_text = df_text.withColumn("path_parts", split(col("file_path"), "/"))
            df_text = df_text.withColumn(
                "lower_parts",
                transform(col("path_parts"), lambda x: spark_lower(x))
            )
            df_text = df_text.withColumn(
                "has_watermark_folder",
                array_contains(col("lower_parts"), SKIP_SUBFOLDER_NAME.lower())
            )

            df_filtered = df_text.filter(~col("has_watermark_folder"))

            if df_filtered.rdd.isEmpty():
                logger.info(f"All JSON files in '{shortcut_name}' are in 'watermark' folder → skipped")
                continue

            df_clean = df_filtered.drop("file_path", "path_parts", "lower_parts", "has_watermark_folder")

            # Parse full JSON as map (top level)
            df_parsed = df_clean.withColumn(
                "json_map",
                from_json(col("value"), MapType(StringType(), StringType()), {"multiLine": "true"})
            )

            # Extract top-level scalar fields
            df_extracted = df_parsed.select(
                "shortcut_name",
                col("json_map.datasetName").alias("dataset_name"),
                col("json_map.datasetTypeName").alias("dataset_type_name"),
                col("json_map.databaseName").alias("database_name"),
                # Get nested objects as JSON strings
                col("json_map.sourceSystemProperties").alias("source_props_json"),
                col("json_map.curatedProperties").alias("curated_props_json")
            )

            # Parse nested JSON strings back into maps
            nested_map_schema = MapType(StringType(), StringType())

            df_extracted = df_extracted \
                .withColumn("source_props", from_json(col("source_props_json"), nested_map_schema)) \
                .withColumn("curated_props", from_json(col("curated_props_json"), nested_map_schema))

            # Now extract the actual fields we need
            df_final = df_extracted \
                .withColumn("source_system_ingest_type", col("source_props.ingestType")) \
                .withColumn("target_load_type", col("curated_props.targetLoadType")) \
                .select(
                    "shortcut_name",
                    "dataset_name",
                    "dataset_type_name",
                    "database_name",
                    "source_system_ingest_type",
                    "target_load_type"
                )

            all_data_frames.append(df_final)

        except Exception as e:
            logger.error(f"Error processing shortcut '{shortcut_name}': {e}")
            continue

    if not all_data_frames:
        logger.info("No valid data extracted from any shortcut.")
        return

    # Union all
    df_combined = all_data_frames[0]
    for df in all_data_frames[1:]:
        df_combined = df_combined.unionByName(df, allowMissingColumns=True)

    record_count = df_combined.count()
    logger.info(f"Extracted {record_count} records. Upserting into {TARGET_FQ}...")

    # Idempotent merge
    DeltaTable.forName(spark, TARGET_FQ).alias("target").merge(
        df_combined.alias("source"),
        "target.shortcut_name = source.shortcut_name AND target.dataset_name = source.dataset_name"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()

    logger.info(f"Successfully merged {record_count} records into {TARGET_FQ}")

    # Show result
    display(spark.sql(f"SELECT * FROM {TARGET_FQ} ORDER BY shortcut_name, dataset_name"))
