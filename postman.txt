def run_load_datasets_metadata():
    ensure_target_table()

    logger.info(f"Loading shortcuts config from {CONFIG_PATH}")
    config = load_shortcuts_config(CONFIG_PATH)
    shortcuts = config.get("shortcuts", [])

    if not shortcuts:
        logger.info("No shortcuts defined in config.")
        return

    all_data_frames = []

    # Import once at the top level for efficiency
    from pyspark.sql.functions import (
        input_file_name, split, transform, lower as spark_lower,
        array_contains, lit, from_json, col
    )
    from pyspark.sql.types import MapType, StringType

    for entry in shortcuts:
        shortcut_name = entry.get("name")
        if not shortcut_name:
            logger.warning("Skipping entry without 'name'")
            continue

        shortcut_folder = f"Files/{shortcut_name}"
        json_pattern = f"{shortcut_folder}/**/*.json"   # Recursive

        logger.info(f"Processing shortcut '{shortcut_name}' → recursive scan: {json_pattern}")

        try:
            # Read each JSON file as a single string (one row per file)
            df_text = spark.read.text(json_pattern)

            if df_text.rdd.isEmpty():
                logger.info(f"No JSON files found under '{shortcut_name}'")
                continue

            # Add file path and shortcut name
            df_text = df_text.withColumn("file_path", input_file_name())
            df_text = df_text.withColumn("shortcut_name", lit(shortcut_name))

            # === Watermark folder skipping (case-insensitive) ===
            df_text = df_text.withColumn("path_parts", split(col("file_path"), "/"))
            df_text = df_text.withColumn(
                "lower_parts",
                transform(col("path_parts"), lambda x: spark_lower(x))
            )
            df_text = df_text.withColumn(
                "has_watermark_folder",
                array_contains(col("lower_parts"), SKIP_SUBFOLDER_NAME.lower())
            )

            df_filtered = df_text.filter(~col("has_watermark_folder"))

            if df_filtered.rdd.isEmpty():
                logger.info(f"All JSON files in '{shortcut_name}' are inside a 'watermark' folder → skipped")
                continue

            # Drop helper columns early
            df_clean = df_filtered.drop("file_path", "path_parts", "lower_parts", "has_watermark_folder")

            # Parse the full JSON content as a flexible map (handles any extra fields)
            df_parsed = df_clean.withColumn(
                "json_map",
                from_json(col("value"), MapType(StringType(), StringType()), {"multiLine": "true"})
            )

            # Extract top-level fields
            df_extracted = df_parsed.select(
                "shortcut_name",
                col("json_map.datasetName").alias("dataset_name"),
                col("json_map.datasetTypeName").alias("dataset_type_name"),
                col("json_map.databaseName").alias("database_name")
            )

            # Extract nested structures as maps first
            df_extracted = df_extracted \
                .withColumn("source_props", col("json_map.sourceSystemProperties")) \
                .withColumn("curated_props", col("json_map.curatedProperties"))

            # Extract deeply nested fields safely
            df_extracted = df_extracted \
                .withColumn("source_system_ingest_type", col("source_props.ingestType")) \
                .withColumn("target_load_type", col("curated_props.targetLoadType"))

            # Final column selection
            df_final = df_extracted.select(
                "shortcut_name",
                "dataset_name",
                "dataset_type_name",
                "database_name",
                "source_system_ingest_type",
                "target_load_type"
            )

            all_data_frames.append(df_final)

        except Exception as e:
            logger.error(f"Error processing shortcut '{shortcut_name}': {e}")
            continue

    if not all_data_frames:
        logger.info("No valid data extracted from any shortcut.")
        return

    # Union all DataFrames
    df_combined = all_data_frames[0]
    for df in all_data_frames[1:]:
        df_combined = df_combined.unionByName(df, allowMissingColumns=True)

    record_count = df_combined.count()
    logger.info(f"Extracted {record_count} dataset records. Upserting into {TARGET_FQ}...")

    # Idempotent MERGE on natural key
    DeltaTable.forName(spark, TARGET_FQ).alias("target").merge(
        df_combined.alias("source"),
        "target.shortcut_name = source.shortcut_name AND target.dataset_name = source.dataset_name"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()

    logger.info(f"Successfully merged {record_count} records into {TARGET_FQ}")

    # Display final result
    display(spark.sql(f"SELECT * FROM {TARGET_FQ} ORDER BY shortcut_name, dataset_name"))
