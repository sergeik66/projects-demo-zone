# ## den_nbk_pdi_001_load_datasets_metadata


import json
import logging
from typing import List, Dict, Any

import yaml
import sempy.fabric as fabric
from pyspark.sql.functions import lit, col, input_file_name, split, transform, lower as spark_lower, array_contains, from_json
from pyspark.sql.types import StructType, StructField, StringType, MapType
from delta.tables import DeltaTable
import fsspec


# In[2]:


# =============================================================================
# Setup
# =============================================================================

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)


# In[3]:


# =============================================================================
# Configuration
# =============================================================================

# Detect PRD vs non-PRD like your original script
ws_name = spark.conf.get("trident.workspace.name", "").upper()
is_prd = "-PRD" in ws_name
config_file_name = "create_shortcuts_config_test"
config_variant = "_prd" if is_prd else ""

workspace_id = fabric.get_workspace_id()
lakehouse_id = fabric.get_lakehouse_id()

CONFIG_PATH = (
    f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/"
    f"{lakehouse_id}/Files/shortcuts_config/"
    f"{config_file_name}{config_variant}.yaml"
)

TARGET_SCHEMA = "catalog"
TARGET_TABLE = "datasets_test"
TARGET_ST = f"{TARGET_SCHEMA}.{TARGET_TABLE}"
SKIP_SUBFOLDER_NAME = "watermark"


# In[4]:


def load_shortcuts_config(path: str) -> Dict[str, Any]:
    """Load YAML config from OneLake using fsspec."""
    storage_options = {
        "account_name": "onelake",
        "account_host": "onelake.dfs.fabric.microsoft.com",
    }
    fs = fsspec.filesystem("abfss", **storage_options)

    if not notebookutils.fs.exists(path):
        raise FileNotFoundError(f"Config file not found at: {path}")

    logger.info(f"Loading config from {path}")
    with fs.open(path, "r") as f:
        return yaml.safe_load(f)


# In[5]:


def ensure_target_table():
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {TARGET_SCHEMA}")
    spark.sql(f"""
            CREATE TABLE IF NOT EXISTS {TARGET_ST} (
                shortcut_name                       STRING,
                dataset_name                        STRING,
                dataset_type_name                   STRING,
                database_name                       STRING,
                dataset_schema                      STRING,
                source_system_ingest_type           STRING,
                target_load_type                    STRING,
                filter_expression                   STRING,
                source_watermark_identifier         STRING
            ) USING DELTA
    """)
    logger.info(f"Ensured table {TARGET_ST} exists.")


# In[6]:


def run_load_datasets_metadata():
    ensure_target_table()

    logger.info(f"Loading shortcuts config from {CONFIG_PATH}")
    config = load_shortcuts_config(CONFIG_PATH)
    shortcuts = config.get("shortcuts", [])

    if not shortcuts:
        logger.info("No shortcuts defined in config.")
        return

    all_data_frames = []

    from pyspark.sql.types import MapType, StringType

    for entry in shortcuts:
        shortcut_name = entry.get("name")
        if not shortcut_name:
            logger.warning("Skipping entry without 'name'")
            continue

        # Only root-level *.json files
        json_pattern = f"Files/{shortcut_name}/*.json"  
        logger.info(f"Processing shortcut '{shortcut_name}' → root files only: {json_pattern}")

        try:
            # Use wholeTextFiles for complete files
            rdd_files = spark.sparkContext.wholeTextFiles(json_pattern)
            if rdd_files.isEmpty():
                logger.info(f"No root-level .json files found in '{shortcut_name}'")
                continue

            df_files = spark.createDataFrame(rdd_files, ["file_path", "value"])
            df_files = df_files.withColumn("shortcut_name", lit(shortcut_name))

            # Safety check: skip if somehow placed inside /watermark/ (should be rare at root)
            df_files = df_files.withColumn("path_parts", split(col("file_path"), "/"))
            df_files = df_files.withColumn(
                "lower_parts", transform(col("path_parts"), lambda x: spark_lower(x))
            )
            df_files = df_files.withColumn(
                "has_watermark_folder", array_contains(col("lower_parts"), "watermark")
            )

            df_filtered = df_files.filter(~col("has_watermark_folder"))

            if df_filtered.rdd.isEmpty():
                logger.info(f"All root files in '{shortcut_name}' are inside watermark → skipped")
                continue

            df_clean = df_filtered.drop("path_parts", "lower_parts", "has_watermark_folder")

            # Parse JSON (same reliable way)
            df_parsed = df_clean.withColumn(
                "json_map",
                from_json(col("value"), MapType(StringType(), StringType()), {"multiLine": "true"})
            )

            df_extracted = df_parsed.select(
                "shortcut_name",
                col("json_map.datasetName").alias("dataset_name"),
                col("json_map.datasetTypeName").alias("dataset_type_name"),
                col("json_map.databaseName").alias("database_name"),
                col("json_map.datasetSchema").alias("dataset_schema"),
                col("json_map.sourceSystemProperties").alias("source_props_json"),
                col("json_map.curatedProperties").alias("curated_props_json")
            )

            nested_map = MapType(StringType(), StringType())
            df_extracted = df_extracted.withColumn(
                "source_props", from_json(col("source_props_json"), nested_map)
            ).withColumn(
                "curated_props", from_json(col("curated_props_json"), nested_map)
            )

            df_final = df_extracted.select(
                col("shortcut_name"),
                spark_lower(col("dataset_name")).alias("dataset_name"),
                spark_lower(col("dataset_type_name")).alias("dataset_type_name"),
                spark_lower(col("database_name")).alias("database_name"),
                spark_lower(col("dataset_schema")).alias("dataset_schema"),
                spark_lower(col("source_props.ingestType")).alias("source_system_ingest_type"),
                spark_lower(col("curated_props.targetLoadType")).alias("target_load_type"),
                spark_lower(col("source_props.filterExpression")).alias("filter_expression"),
                spark_lower(col("source_props.sourceWatermarkIdentifier")).alias("source_watermark_identifier")
            )

            all_data_frames.append(df_final)

        except Exception as e:
            logger.error(f"Error processing '{shortcut_name}': {str(e)}")
            continue

    if not all_data_frames:
        logger.info("No valid root-level .json files extracted from any shortcut.")
        return

    df_combined = all_data_frames[0]
    for df in all_data_frames[1:]:
        df_combined = df_combined.unionByName(df, allowMissingColumns=True)

    # Final safety deduplication
    df_combined = df_combined.dropDuplicates(["shortcut_name", "dataset_name", "dataset_schema", "dataset_type_name", "database_name", "source_system_ingest_type", "target_load_type"])

    record_count = df_combined.count()
    logger.info(f"Extracted & deduplicated {record_count} root-level records.")

    # Merge
    DeltaTable.forName(spark, TARGET_ST).alias("target").merge(
        df_combined.alias("source"),
        "target.shortcut_name = source.shortcut_name AND target.dataset_name = source.dataset_name"
    ).whenMatchedUpdateAll() \
     .whenNotMatchedInsertAll() \
     .execute()

    logger.info(f"Successfully merged {record_count} records into {TARGET_ST}")
    display(spark.sql(f"SELECT * FROM {TARGET_ST} ORDER BY shortcut_name, dataset_name LIMIT 1000"))


# In[7]:


run_load_datasets_metadata()

