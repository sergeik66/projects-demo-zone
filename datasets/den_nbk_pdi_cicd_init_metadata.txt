#!/usr/bin/env python
# coding: utf-8
"""
deploy_catalog_metastore.py
Production-ready metastore deployment + config generator for IIS ingestion pipeline.
"""

import logging
import os
import json
from datetime import datetime
from typing import Dict, List, Optional
from collections import defaultdict

import fsspec
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_date, current_timestamp, date_format
import sempy.fabric as fabric

# ----------------------------- Config -----------------------------
class Config:
    SCHEMA_NAME = os.getenv("CATALOG_SCHEMA", "catalog")
    FEED_FILE_NAME = "iis_curated_feed"
    DATASET_CURATED_FILE_NAME = "dataset_curated_list"
    RAW_LAKEHOUSE = os.getenv("RAW_LAKEHOUSE", "den_lhw_dpr_001_raw_files")
    CURATED_LAKEHOUSE = os.getenv("CURATED_LAKEHOUSE", "den_lhw_scu_001_iis_curated")
    IS_PRD = "-PRD" in spark.conf.get("trident.workspace.name", "").upper()  # type: ignore[name-defined]

    @property
    def feed_variant(self) -> str:
        return "_prd" if self.IS_PRD else ""

    @property
    def feed_path(self) -> str:
        ws_id = fabric.get_workspace_id()
        lh_id = fabric.get_lakehouse_id()
        return (
            f"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/"
            f"{lh_id}/Files/feeds/{self.FEED_FILE_NAME}{self.feed_variant}.json"
        )

    @property
    def curated_dataset_list_path(self) -> str:
        ws_id = fabric.get_workspace_id()
        lh_id = fabric.get_lakehouse_id()
        return (
            f"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/"
            f"{lh_id}/Files/feeds/{self.DATASET_CURATED_FILE_NAME}.json"
        )

    @property
    def datasets_path(self) -> str:
        ws_id = fabric.get_workspace_id()
        lh_id = fabric.get_lakehouse_id()
        return (
            f"abfss://{ws_id}@onelake.dfs.fabric.microsoft.com/"
            f"{lh_id}/Files/datasets/iis_curated/"
        )


CONFIG = Config()
STORAGE_OPTIONS = {"account_name": "onelake", "account_host": "onelake.dfs.fabric.microsoft.com"}

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s")
logger = logging.getLogger(__name__)
spark: SparkSession = spark  # type: ignore[name-defined]  # Fabric global


# ----------------------------- Helpers -----------------------------
def create_schema(schema_name: str) -> None:
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
    spark.sql(f"USE {schema_name}")
    logger.info(f"Schema {schema_name} ready")


def create_and_upsert_datasource_list() -> None:
    ddl = f"""
    CREATE TABLE IF NOT EXISTS {CONFIG.SCHEMA_NAME}.datasource_list (
        datasource_key INT,
        datasource_name STRING,
        datasource_type STRING,
        extraction_method STRING,
        datasource_connection_name STRING,
        contact_email STRING,
        datasource_notes STRING,
        dl_is_current_flag BOOLEAN,
        dl_row_expiration_date DATE,
        dl_row_effective_date DATE,
        dl_row_update_timestamp TIMESTAMP,
        dl_row_insert_timestamp TIMESTAMP
    ) USING DELTA COMMENT 'List of data sources for SACDP ingestion.';
    """
    spark.sql(ddl)

    seed_df = spark.createDataFrame([(
        "IIS", "SQL Server", "SQL Query",
        "ONP-DEV-SQLSERVER-SACDP-CPL-YAZSQLDAILY",
        "tony.thornton@guard.com",
        "IIS source"
    )], ["datasource_name", "datasource_type", "extraction_method",
         "datasource_connection_name", "contact_email", "datasource_notes"])

    merge_sql = f"""
    MERGE INTO {CONFIG.SCHEMA_NAME}.datasource_list tgt
    USING (
        SELECT ROW_NUMBER() OVER (ORDER BY datasource_name) AS datasource_key, *
        FROM (SELECT *, true AS dl_is_current_flag, current_date() AS dl_row_effective_date,
                     DATE'9999-01-01' AS dl_row_expiration_date,
                     current_timestamp() AS dl_row_update_timestamp,
                     current_timestamp() AS dl_row_insert_timestamp
              FROM seed_df_temp)
    ) src
    ON tgt.datasource_name = src.datasource_name
    WHEN MATCHED THEN UPDATE SET *
    WHEN NOT MATCHED THEN INSERT *
    """
    seed_df.createOrReplaceTempView("seed_df_temp")
    spark.sql(merge_sql)
    logger.info("datasource_list seeded (MERGE)")


# (create_dataset_list, create_dataset_pii_list, create_dataset_column_list are unchanged except schema from CONFIG)


def create_feed_json() -> None:
    query = f"""
    SELECT
        dataset_name AS fileName,
        CONCAT('iis_', database_name) AS sourceConfigFolderName,
        database_name AS databaseName
    FROM {CONFIG.SCHEMA_NAME}.curated_datasets_subdomains
    WHERE dataset_type_name = 'database' AND dataset_schema = 'dbo'
    GROUP BY database_name, dataset_schema, dataset_name
    """
    df = spark.sql(query)
    datasets = df.toPandas().to_dict("records") if df.count() > 0 else []

    payload = {
        "feedName": CONFIG.FEED_FILE_NAME,
        "feedDescription": "IIS curated feed",
        "workLoadType": "dp-spark",
        "skipProductLoad": False,
        "dataDeduplicationAlert": False,
        "deduplicationAlertTemplate": "deduplication_msg",
        "datasetsConfig": [{"datasetType": "database", "datasets": datasets}]
    }

    fs = fsspec.filesystem("abfss", **STORAGE_OPTIONS)
    with fs.open(CONFIG.feed_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)
    logger.info(f"Feed JSON → {CONFIG.feed_path} ({len(datasets)} datasets)")


# create_dataset_list_from_feed → unchanged logic (grouped IN() list)


def create_dataset_files(dataset_filter: Optional[List[str]] = None) -> None:
    """Generate per-dataset JSON configs. Processes all datasets unless filter provided."""
    # ... (load pii_df, column_df, dataset_df)

    filter_clause = f" AND dataset_name IN ({','.join([repr(d) for d in dataset_filter])})" if dataset_filter else ""
    # Build queries with filter_clause ...

    datasets = column_df.select("dataset_name").distinct().collect()
    for row in datasets:
        dataset_name = row[0]
        # ... (rest of logic identical, but now uses CONFIG.RAW_LAKEHOUSE etc.)

        # Write
        out_path = f"{CONFIG.datasets_path}iis_{database_name.upper()}/{dataset_name}.json"
        # ... json.dump

    logger.info(f"Generated {len(datasets)} dataset JSON files")


def main(dataset_filter: Optional[List[str]] = None, run_me: bool = True):
    if not run_me:
        return
    start = datetime.now()
    logger.info("=== deploy_catalog_metastore START ===")

    try:
        create_schema(CONFIG.SCHEMA_NAME)
        create_and_upsert_datasource_list()
        create_dataset_list()
        create_dataset_pii_list()
        create_dataset_column_list()

        create_feed_json()
        create_dataset_list_from_feed(CONFIG.feed_path, CONFIG.curated_dataset_list_path, STORAGE_OPTIONS)
        create_dataset_files(dataset_filter=dataset_filter)

        logger.info(f"Completed in {datetime.now() - start}")
    except Exception as e:
        logger.error("Failed", exc_info=True)
        raise


if __name__ == "__main__":
    # Example usage:
    # main(dataset_filter=["wclocnam", "agency"])   # selective
    main()
