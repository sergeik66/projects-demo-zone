def ingest(self, source_data: DataFrame):
        base_data = DeltaTable.forPath(spark, self.table_path)
        merge_predicate = self._create_merge_predicate(self._candidate_keys)
        upd_col = self._get_upd_col(df=source_data, exclusions=["dl_createddateutc"])

        # Check if source data contains CDC columns
        is_cdc = "__$operation" in source_data.columns

        if is_cdc:
            # Handle CDC: inserts (__$operation = 2), updates (__$operation = 4), deletes (__$operation = 1)
            cdc_exclusions = [
                "dl_createddateutc",
                "__$start_lsn",
                "__$end_lsn",
                "__$seqval",
                "__$operation",
                "__$update_mask",
                "row_num"
            ]
            upd_col_cdc = self._get_upd_col(df=source_data, exclusions=cdc_exclusions)
            (
                base_data.alias("bd").merge(
                    source=source_data.alias("sd"),
                    condition=merge_predicate
                )
                .withSchemaEvolution()
                .whenMatchedUpdate(
                    condition="sd.__$operation IN (2, 4) AND bd.dl_rowhash <> sd.dl_rowhash",
                    set=upd_col_cdc
                )
                .whenMatchedUpdate(
                    condition="sd.__$operation = 1",
                    set={
                        "dl_is_deleted": lit(1),
                        "dl_iscurrent": lit(0),
                        "dl_lastmodifiedutc": lit(self.batch_time)
                    }
                )
                .whenNotMatchedInsert(
                    condition="sd.__$operation IN (2, 4)",
                    values=upd_col_cdc
                )
                .execute()
            )
        else:
            # Non-CDC merge
            (
                base_data.alias("bd").merge(
                    source=source_data.alias("sd"),
                    condition=merge_predicate
                )
                .withSchemaEvolution()
                .whenMatchedUpdate(condition="bd.dl_rowhash <> sd.dl_rowhash", set=upd_col)
                .whenNotMatchedInsertAll()
                .execute()
            )

(
                base_data.alias("bd").merge(
                    source=source_data.alias("sd"),
                    condition=merge_predicate
                )
                .withSchemaEvolution()
                .whenMatchedUpdate(
                    condition=(col("sd.__$operation").isin(2, 4) & (col("bd.dl_rowhash") != col("sd.dl_rowhash"))),
                    set=upd_col_cdc
                )
                .whenMatchedUpdate(
                    condition=col("sd.__$operation") == 1,
                    set={
                        "dl_is_deleted": lit(1),
                        "dl_iscurrent": lit(0),
                        "dl_lastmodifiedutc": lit(self.batch_time)
                    }
                )
                .whenNotMatchedInsert(
                    condition=col("sd.__$operation").isin(2, 4),
                    values=upd_col_cdc
                )
                .execute()

AnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve dl_is_deleted in UPDATE clause given columns sd.code, sd.primeseq, sd.state, sd.effdate, sd.classcode, sd.seqcode, sd.territory, sd.exmod, sd.minprem, sd.exposure, sd.premium, sd.rate, sd.xcluamt, sd.losscons, sd.form, sd.paragraph, sd.volcomp, sd.hazard, sd.cvrg, sd.disctype, sd.classuffix, sd.diffprem, sd.diffrate, sd.rateeffdate, sd.losscost, sd.__operation, sd.__start_lsn, sd.__seqval, sd.dl_rowhash, sd.dl_partitionkey, sd.dl_iscurrent, sd.dl_recordstartdateutc, sd.dl_recordenddateutc, sd.dl_createddateutc, sd.dl_lastmodifiedutc, sd.dl_sourcefilepath, sd.dl_sourcefilename, sd.dl_eltid, sd.dl_runid
