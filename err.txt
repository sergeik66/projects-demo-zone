# Load version 9 and version 10 of the Delta table
df_version_10 = spark.read.format("delta").option("versionAsOf", 10).load("path/to/delta_table")
df_version_9 = spark.read.format("delta").option("versionAsOf", 9).load("path/to/delta_table")

# Identify updated records by joining on a key column (e.g., 'id')
# Replace 'id' with your actual key column and 'some_column' with columns to check for updates
updated_records = df_version_10.join(df_version_9, "id", "inner") \
    .where(
        # Check if any non-key column differs (adjust based on your schema)
        (df_version_10.some_column != df_version_9.some_column) |
        (df_version_10.some_column.isNull() != df_version_9.some_column.isNull())
    ) \
    .select(df_version_10["*"])  # Select all columns from version 10 (updated records)

# Show the updated records
updated_records.show(truncate=False)
