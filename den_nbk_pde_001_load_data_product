import json
from datetime import datetime
from uuid import uuid4
from spark_engine.sparkengine import SparkEngine
from spark_engine.common.lakehouse import LakehouseManager
import notebookutils as nu

feed_name = 'claims_bop_demo'
run_id = 'f4b58b89-aaa7-45d9-b301-82fe25c28de9'
elt_id = '016d62c1-d885-4936-8d29-80b09868f589'
product_name = 'BOP'
file_name = 'BOPClaimSummary.yaml'
model_config_folder_name = 'data_product/claims_bop'
source_system = 'Curated'
invocation_id = 'f4b58b89-aaa7-45d9-b301-82fe25c28de9'
elt_start_date_time = "12/12/2024 13:19:26"

def get_current_timestamp() -> object:
    return datetime.utcnow()

def get_file_location_url(lakehouse_name,file_relative_path) -> str:

    lakehouse_manager = LakehouseManager(lakehouse_name=lakehouse_name)
    lakehouse_files_path = f"{lakehouse_manager.lakehouse_path}/Files"

    return f"{lakehouse_files_path}/{file_relative_path}"

def send_message_to_logs(message_metadata: object, log_file_name: str) -> object:
    message = {
        "product_name": product_name,
        "feed_name": feed_name,
        "dataset_name": file_name,
        "source_system": source_system,
        "metadata": message_metadata,
        "zone": zone_name,
        "stage": stage_name,
        "orchestration_tool": "spark",
        "zone_start_date_time": str(processing_start_time),
        "zone_end_date_time": str(get_current_timestamp()),
        "elt_id": elt_id,
        "run_id": run_id,
        "invocation_id": invocation_id
    }

    output_message = json.dumps(message)

    # save message content to a log file later processesing
    try:   
        nu.fs.put(log_file_name, output_message, True)
    except Exception as error:
        raise error
def process_data(product_config_path, product_name, feed_name, file_name, elt_id, run_id, processing_start_time, log_file_name):
    try:
        data = (
            SparkEngine.transform(product_config_path)
            .configure_transform(
                product_name=product_name,
                feed_name=feed_name,
                dataset_name=file_name
            )
            .start_transform(elt_id=elt_id, run_id=run_id)
            .metrics()
        )
    except Exception as error:
        data = {
            "ingestion": {
                "error_message": str(error),
                "startTime": str(processing_start_time)
            }
        }
        print("Exception occurred while processing the data: ", error)
        raise error
    finally:
        message_metadata = {"runOutput": data}
        send_message_to_logs(message_metadata, log_file_name)

    return data
processing_start_time = elt_start_date_time

logs_lakehouse_name = "den_lhw_pdi_001_observability"
product_config_lakehouse_name = "den_lhw_pdi_001_metadata"

config_file_relative_path = f"{model_config_folder_name}/{file_name}"
log_file_relative_path = f"Metadata_Logs/{uuid4()}.json"

# for backward compatability
# check if additional parameters were passed:
requred_params = ["workspace_id","lh_metadata_id","lh_observability_id"]
missing_params = []

for param in requred_params:
    if param not in locals() or eval(param) is None or eval(param) == '':
        missing_params.append(param)
# construct paths for log and yaml files
if missing_params:
    log_file_name = get_file_location_url(logs_lakehouse_name, log_file_relative_path)
    product_config_path = get_file_location_url(product_config_lakehouse_name, config_file_relative_path)
    print("Constructing abfss path with LakehouseManager class")
else:
    log_file_name = f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lh_observability_id}/Files/{log_file_relative_path}"
    product_config_path = f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lh_metadata_id}/Files/{config_file_relative_path}"
    print(f"Constructing abfss path with additional parameters:workspace_id={workspace_id},lh_observability_id={lh_observability_id},lh_metadata_id={lh_metadata_id}")

# check if parameter was passed:
if 'zone' in locals():
    zone_name = zone
    stage_name = "Share"
else:
    zone_name = "Product"
    stage_name = "Transformation"
