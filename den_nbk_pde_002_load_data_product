import json
from datetime import datetime
from uuid import uuid4
from spark_engine.sparkengine import SparkEngine
from spark_engine.common.lakehouse import LakehouseManager
import notebookutils as nu
from concurrent.futures import ThreadPoolExecutor, as_completed

# Existing variables
feed_name = 'claims_bop_demo'
run_id = 'f4b58b89-aaa7-45d9-b301-82fe25c28de9'
elt_id = '016d62c1-d885-4936-8d29-80b09868f589'
product_name = 'BOP'
source_system = 'Curated'
invocation_id = 'f4b58b89-aaa7-45d9-b301-82fe25c28de9'
elt_start_date_time = "12/12/2024 13:19:26"

# JSON payload (for demonstration; replace with actual loading mechanism if needed)
json_payload = '''
{
    "loadGroupA": [
        {
            "files": [
                {
                    "fileName": "dim_catastrophe",
                    "modelConfigFolderName": "demo_product"
                },
                {
                    "fileName": "dim_date",
                    "modelConfigFolderName": "demo_product"
                }
            ]
        }
    ],
    "loadGroupB": [
        {
            "files": []
        }
    ],
    "loadGroupC": [
        {
            "files": []
        }
    ]
}
'''

def get_current_timestamp() -> object:
    return datetime.utcnow()

def get_file_location_url(lakehouse_name, file_relative_path) -> str:
    lakehouse_manager = LakehouseManager(lakehouse_name=lakehouse_name)
    lakehouse_files_path = f"{lakehouse_manager.lakehouse_path}/Files"
    return f"{lakehouse_files_path}/{file_relative_path}"

def send_message_to_logs(message_metadata: object, log_file_name: str) -> object:
    message = {
        "product_name": product_name,
        "feed_name": feed_name,
        "dataset_name": file_name,
        "source_system": source_system,
        "metadata": message_metadata,
        "zone": zone_name,
        "stage": stage_name,
        "orchestration_tool": "spark",
        "zone_start_date_time": str(processing_start_time),
        "zone_end_date_time": str(get_current_timestamp()),
        "elt_id": elt_id,
        "run_id": run_id,
        "invocation_id": invocation_id
    }
    output_message = json.dumps(message)
    try:
        nu.fs.put(log_file_name, output_message, True)
    except Exception as error:
        raise error

def process_data(product_config_path, product_name, feed_name, file_name, elt_id, run_id, processing_start_time, log_file_name):
    max_retries = 3
    retry_delay = 60  # seconds

    for attempt in range(max_retries):
        try:
            data = (
                SparkEngine.transform(product_config_path)
                .configure_transform(
                    product_name=product_name,
                    feed_name=feed_name,
                    dataset_name=file_name
                )
                .start_transform(elt_id=elt_id, run_id=run_id)
                .metrics()
            )
            return data  # Success, exit retry loop

        except Exception as error:
            # Check if the error is related to status code 429
            is_429 = False
            error_message = str(error).lower()
            # Check for 429 in message or exception attributes (adjust based on spark_engine behavior)
            if "429" in error_message or "too many requests" in error_message:
                is_429 = True
            elif hasattr(error, 'status_code') and error.status_code == 429:
                is_429 = True
            elif hasattr(error, 'response') and hasattr(error.response, 'status_code') and error.response.status_code == 429:
                is_429 = True

            if is_429 and attempt < max_retries - 1:
                print(f"Received 429 for {file_name}, attempt {attempt + 1}/{max_retries}. Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                continue  # Retry after delay
            else:
                # Either not a 429 or max retries reached
                data = {
                    "ingestion": {
                        "error_message": str(error),
                        "startTime": str(processing_start_time)
                    }
                }
                print(f"Exception occurred while processing the data for {file_name}: {error}")
                raise error  # Re-raise the error for outer handling
        finally:
            message_metadata = {"runOutput": data if 'data' in locals() else {}}
            send_message_to_logs(message_metadata, log_file_name)
def get_spark_max_workers():
    try:
        # Option 1: Check environment variables (common in Databricks/Fabric)
        spark_cores = os.environ.get("SPARK_EXECUTOR_CORES")  # Set in cluster config
        if spark_cores:
            total_cores = int(spark_cores) * int(os.environ.get("SPARK_EXECUTOR_INSTANCES", 1))
            max_workers = max(1, total_cores - 1)  # Subtract 1 for headroom
            print(f"Detected {total_cores} Spark cores from environment, setting max_workers to {max_workers}")
            return max_workers

        # Option 2: Fallback to a heuristic based on CPU count (if available)
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        max_workers = max(1, cpu_count - 1)
        print(f"Using CPU count {cpu_count}, setting max_workers to {max_workers}")
        return max_workers

    except Exception as e:
        print(f"Could not determine Spark cores, defaulting to max_workers=3: {e}")
        return 3  # Fallback to a safe default

max_workers = get_spark_max_workers()

# Lakehouse names
logs_lakehouse_name = "den_lhw_pdi_001_observability"
product_config_lakehouse_name = "den_lhw_pdi_001_metadata"

# Check for additional parameters
required_params = ["workspace_id", "lh_metadata_id", "lh_observability_id"]
missing_params = [param for param in required_params if param not in locals() or eval(param) is None or eval(param) == '']

# Zone and stage settings
if 'zone' in locals():
    zone_name = zone
    stage_name = "Share"
else:
    zone_name = "Product"
    stage_name = "Transformation"

# Parse JSON payload
payload = json.loads(json_payload)

# Process each LoadGroup sequentially, but process files in parallel
processing_start_time = elt_start_date_time

def process_file(file_info, missing_params):
    """Helper function to process a single file."""
    file_name = file_info["fileName"]
    model_config_folder_name = file_info["modelConfigFolderName"]

    # Construct file paths
    config_file_relative_path = f"{model_config_folder_name}/{file_name}.yaml"
    log_file_relative_path = f"Metadata_Logs/{uuid4()}.json"

    if missing_params:
        log_file_name = get_file_location_url(logs_lakehouse_name, log_file_relative_path)
        product_config_path = get_file_location_url(product_config_lakehouse_name, config_file_relative_path)
        print(f"Constructing abfss path with LakehouseManager class for {file_name}")
    else:
        log_file_name = f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lh_observability_id}/Files/{log_file_relative_path}"
        product_config_path = f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lh_metadata_id}/Files/{config_file_relative_path}"
        print(f"Constructing abfss path with additional parameters for {file_name}: workspace_id={workspace_id}, lh_observability_id={lh_observability_id}, lh_metadata_id={lh_metadata_id}")

    # Process the data for the current file
    print(f"Processing file: {file_name}")
    return process_data(
        product_config_path=product_config_path,
        product_name=product_name,
        feed_name=feed_name,
        file_name=file_name,
        elt_id=elt_id,
        run_id=run_id,
        processing_start_time=processing_start_time,
        log_file_name=log_file_name
    )

for load_group_key, load_groups in payload.items():
    print(f"Processing {load_group_key}")
    for load_group in load_groups:
        files = load_group.get("files", [])
        if not files:
            print(f"No files to process in {load_group_key}")
            continue

        # Process files in parallel using ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers as needed
            future_to_file = {
                executor.submit(process_file, file_info, missing_params): file_info["fileName"]
                for file_info in files
            }
            for future in as_completed(future_to_file):
                file_name = future_to_file[future]
                try:
                    result = future.result()
                    print(f"Completed processing {file_name} with result: {result}")
                except Exception as e:
                    print(f"Error processing {file_name}: {e}")
