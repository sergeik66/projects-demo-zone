Below is a **complete, ready-to-run Fabric Notebook** (`.ipynb` JSON format) that:

- Creates an **optimized version** of your `mgnotrans` table  
- Applies **SCD Type 1 best practices**  
- **Partitions by `enteredon_year`, `enteredon_month`**  
- **Z-Orders by `transno`** (for fast joins)  
- **Drops unnecessary SCD2 columns** (`dl_iscurrent`, `dl_recordstartdateutc`, etc.)  
- Runs `OPTIMIZE`, `ANALYZE`, and validation  
- Includes **SQL Endpoint test queries**  

---

### How to Use
1. Open **Microsoft Fabric** → **Lakehouse** → **New Notebook**  
2. Click **"Import"** → paste this JSON into a `.ipynb` file or upload it  
3. Run all cells in order  
4. After completion, **swap** the old table with the new one (optional)

---

### Full Notebook JSON (Copy & Save as `Optimize_mgnotrans_Type1.ipynb`)

```json
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Optimize `mgnotrans` Delta Table (SCD Type 1)\n",
        "\n",
        "**Goal**: 95M-row table → fast incremental MERGE + fast fact joins\n",
        "\n",
        "**Actions**:\n",
        "- Partition by `enteredon_year`, `enteredon_month`\n",
        "- Z-Order by `transno` (join key)\n",
        "- Drop SCD2 columns (`dl_iscurrent`, `dl_recordstart/enddateutc`)\n",
        "- Compact files + update stats\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 1: Check current table stats\n",
        "spark.sql(\"\"\"\n",
        "DESCRIBE DETAIL spark_catalog.chimcobldhq2ak34ehgmsp2ge9hiqgrfdlml0p3k9hsn4bah84im8pbebtm6gtqvedhnanpg60olus3fdhkm6uavcdqn4obkcli2as3fdhkm6u8.mgnotrans\n",
        "\"\"\").select(\"numFiles\", \"sizeInBytes\", \"partitionColumns\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 2: Create optimized table (Type 1)\n",
        "spark.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE mgnotrans_opt\n",
        "USING DELTA\n",
        "PARTITIONED BY (enteredon_year, enteredon_month)\n",
        "LOCATION 'abfss://cd63feda-59d3-40b2-87da-4b2519547f70@onelake.dfs.fabric.microsoft.com/19d6879b-5a1e-44ef-a07d-a23bdce14e0e/Tables/policy/mgnotrans_opt'\n",
        "AS SELECT\n",
        "    id, code, trancnt, state, typeid, transcode, reason,\n",
        "    additional_info, extra, effdate,\n",
        "    enteredon, transno,\n",
        "    dl_watermark, dl_rowhash, dl_partitionkey,\n",
        "    YEAR(enteredon) AS enteredon_year,\n",
        "    MONTH(enteredon) AS enteredon_month\n",
        "FROM spark_catalog.chimcobldhq2ak34ehgmsp2ge9hiqgrfdlml0p3k9hsn4bah84im8pbebtm6gtqvedhnanpg60olus3fdhkm6uavcdqn4obkcli2as3fdhkm6u8.mgnotrans\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 3: Z-Order by join key (transno)\n",
        "spark.sql(\"\"\"\n",
        "OPTIMIZE mgnotrans_opt ZORDER BY (transno)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 4: Compact small files\n",
        "spark.sql(\"OPTIMIZE mgnotrans_opt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 5: Update statistics\n",
        "spark.sql(\"ANALYZE TABLE mgnotrans_opt COMPUTE STATISTICS FOR ALL COLUMNS\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 6: Validate row count\n",
        "display(spark.sql(\"\"\"\n",
        "SELECT \n",
        "  (SELECT COUNT(*) FROM spark_catalog.chimcobldhq2ak34ehgmsp2ge9hiqgrfdlml0p3k9hsn4bah84im8pbebtm6gtqvedhnanpg60olus3fdhkm6uavcdqn4obkcli2as3fdhkm6u8.mgnotrans) AS old_count,\n",
        "  (SELECT COUNT(*) FROM mgnotrans_opt) AS new_count\n",
        "\"\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 7: Show new table stats\n",
        "spark.sql(\"DESCRIBE DETAIL mgnotrans_opt\").select(\"numFiles\", \"sizeInBytes\", \"partitionColumns\").show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Optional: Replace Original Table\n",
        "Run only after validation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cell 8: (OPTIONAL) Swap tables\n",
        "# spark.sql(\"DROP TABLE IF EXISTS mgnotrans\")\n",
        "# spark.sql(\"ALTER TABLE mgnotrans_opt RENAME TO mgnotrans\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Test in SQL Endpoint\n",
        "Copy & run in **SQL Analytics Endpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%sql\n",
        "-- Test partition pruning\n",
        "SET SHOWPLAN_XML ON;\n",
        "GO\n",
        "SELECT COUNT(*) \n",
        "FROM mgnotrans_opt \n",
        "WHERE enteredon_year = 2025 AND enteredon_month = 10;\n",
        "GO\n",
        "SET SHOWPLAN_XML OFF;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Done!** Your table is now:\n",
        "- **Partitioned for incremental loads**\n",
        "- **Z-Ordered for fast joins**\n",
        "- **Lean & fast**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
```

---

### Save & Run Instructions

1. **Save** the above JSON as:  
   `Optimize_mgnotrans_Type1.ipynb`

2. **Upload to Fabric**:
   - Open your Lakehouse
   - Click **"New" → "Notebook" → "Import"**
   - Upload the `.ipynb` file

3. **Run all cells** (Shift+Enter)

4. **After Cell 7**, verify:
   - `old_count == new_count`
   - `numFiles` decreased
   - `sizeInBytes` reduced

5. **Uncomment Cell 8** to replace the old table (only after validation)

---

### Expected Outcome

| Metric | Before | After |
|-------|--------|-------|
| Partitions | 15 (`dl_partitionkey`) | ~72 (`year/month`) |
| Files | 8,000+ | ~200–300 |
| Size | ~45 GB | **~25–30 GB** |
| MERGE (1 day) | 20+ min | **2–4 min** |
| Fact Join | 8 min | **1–2 min** |

---

**Let me know when you’ve run it — I’ll help interpret the stats or build the maintenance pipeline!**
