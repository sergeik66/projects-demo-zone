import pgpy
import fsspec
import os
import keyring
from azure.keyvault.secrets import SecretClient
from azure.identity import ClientSecretCredential
from spark_engine.common.email_util import get_secret_notebookutils
from pgp import PGP  # Adjust import based on your module structure

# Service principal credentials (from environment variables)
client_id = os.getenv("AZURE_CLIENT_ID", "<your_client_id>")
client_secret = os.getenv("AZURE_CLIENT_SECRET", "<your_client_secret>")
tenant_id = os.getenv("AZURE_TENANT_ID", "<your_tenant_id>")

# Generate a new RSA key pair
try:
    key = pgpy.PGPKey.new(pgpy.constants.PubKeyAlgorithm.RSAEncryptOrSign, 2048)
    if key is None:
        raise ValueError("Failed to generate PGP key: key is None")
except Exception as e:
    print(f"Error generating PGP key: {str(e)}")
    raise

# Add a User ID to the key
try:
    uid = pgpy.PGPUID.new("Test User", email="test@example.com")
    key.add_uid(
        uid,
        usage={
            pgpy.constants.KeyFlags.Sign,
            pgpy.constants.KeyFlags.EncryptCommunications,
            pgpy.constants.KeyFlags.EncryptStorage
        },
        hashes=[pgpy.constants.HashAlgorithm.SHA256],
        ciphers=[pgpy.constants.SymmetricKeyAlgorithm.AES256],
        compression=[pgpy.constants.CompressionAlgorithm.ZIP]
    )
except Exception as e:
    print(f"Error adding User ID to key: {str(e)}")
    raise

# Protect the private key with a passphrase
passphrase = "my-passphrase"
try:
    key.protect(passphrase, pgpy.constants.SymmetricKeyAlgorithm.AES256, None)
    print("Private key protected with passphrase")
except Exception as e:
    print(f"Failed to protect private key: {str(e)}")
    raise

# Extract the public and private keys as ASCII-armored strings
try:
    MOCK_PUBLIC_KEY = str(key.pubkey)  # ASCII-armored public key
    MOCK_PRIVATE_KEY = str(key)        # ASCII-armored private key (passphrase-protected)
except Exception as e:
    print(f"Error extracting keys: {str(e)}")
    raise

# Optionally save the keys to files for debugging or reuse
try:
    with open('mock_public_key.asc', 'w') as pub_file:
        pub_file.write(MOCK_PUBLIC_KEY)
    with open('mock_private_key.asc', 'w') as priv_file:
        priv_file.write(MOCK_PRIVATE_KEY)
except Exception as e:
    print(f"Error saving keys to files: {str(e)}")

# Store public key in Azure Key Vault
key_vault_name = "my-key-vault"
public_key_secret_name = "mock-public-key-secret"

vault_url = f"https://{key_vault_name}.vault.azure.net"
credential = ClientSecretCredential(
    tenant_id=tenant_id,
    client_id=client_id,
    client_secret=client_secret
)
secret_client = SecretClient(vault_url=vault_url, credential=credential)

try:
    secret_client.set_secret(public_key_secret_name, MOCK_PUBLIC_KEY)
    print(f"Successfully set secret {public_key_secret_name} in {key_vault_name}")
except Exception as e:
    print(f"Failed to set public key secret: {str(e)}")
    raise

# Store private key in keyring
keyring_service = "pgp_private_key"
keyring_username = "fabric_user"
try:
    keyring.set_password(keyring_service, keyring_username, MOCK_PRIVATE_KEY)
    print(f"Successfully set private key in keyring under service {keyring_service}, username {keyring_username}")
except Exception as e:
    print(f"Failed to set private key in keyring: {str(e)}")
    raise

# Decrypt file function using keyring for private key
def decrypt_file(
    input_folder: str,
    output_path: str,
    pgp_enabled: bool,
    key_vault_name: str = None,
    public_key_secret: str = None,
    passphrase: str = None
) -> None:
    """
    Decrypt all files with .pgp extension in the input folder using PGP if pgp_enabled is True and required parameters are provided.
    
    Args:
        input_folder: Path to the folder containing encrypted files (e.g., .pgp files).
        output_path: Directory path for the decrypted output files.
        pgp_enabled: Flag to enable/disable PGP decryption.
        key_vault_name: Name of the key vault for retrieving public key secret (optional).
        public_key_secret: Secret name for the public key (optional).
        passphrase: Passphrase for the private key (optional).
    
    Raises:
        ValueError: If pgp_enabled is True but key_vault_name or public_key_secret is None,
                    or if no .pgp files are found in the input folder.
        FileNotFoundError: If the input folder does not exist.
        IOError: If decryption or file operations fail for any file.
        keyring.errors.KeyringError: If keyring fails to retrieve the private key.
    """
    if not pgp_enabled:
        print("PGP decryption is disabled. Skipping decryption.")
        return
    
    if key_vault_name is None or public_key_secret is None:
        raise ValueError("key_vault_name and public_key_secret must be provided when pgp_enabled is True.")

    # Initialize fsspec filesystem for OneLake
    try:
        fs = fsspec.filesystem(
            "abfss",
            account_name="onelake",
            account_host="onelake.dfs.fabric.microsoft.com"
        )
    except Exception as e:
        print(f"Error initializing fsspec filesystem: {str(e)}")
        raise

    # Check if the input folder exists
    if not fs.exists(input_folder):
        raise FileNotFoundError(f"Input folder not found: {input_folder}")

    # List files in the input folder and filter for .pgp extension
    try:
        files = [f for f in fs.ls(input_folder, detail=False) if f.lower().endswith(".pgp")]
        if not files:
            raise ValueError(f"No .pgp files found in {input_folder}")
    except Exception as e:
        print(f"Error listing files in input folder: {str(e)}")
        raise

    # Retrieve private key from keyring
    try:
        private_key = keyring.get_password(keyring_service, keyring_username)
        if private_key is None:
            raise ValueError(f"No private key found in keyring for service {keyring_service}, username {keyring_username}")
    except keyring.errors.KeyringError as e:
        print(f"Failed to retrieve private key from keyring: {str(e)}")
        raise

    # Initialize PGP class
    try:
        pgp = PGP(
            key_vault_name=key_vault_name,
            public_key_secret=public_key_secret,
            private_key=private_key
        )
    except Exception as e:
        print(f"Error initializing PGP class: {str(e)}")
        raise
    
    # Decrypt each .pgp file
    for input_file in files:
        try:
            pgp.decrypt_file(
                input_file=input_file,
                output_path=output_path,
                passphrase=passphrase
            )
            print(f"File {input_file} decrypted successfully to {output_path}")
        except Exception as e:
            print(f"Failed to decrypt {input_file}: {str(e)}")
            raise IOError(f"Failed to decrypt {input_file}: {str(e)}")

# Example usage in the notebook
pgp_enabled = True
key_vault_name = "my-key-vault"
public_key_secret = "mock-public-key-secret"
passphrase = "my-passphrase"

try:
    decrypt_file(
        input_folder="abfss://ab08da5e-0f71-423b-a811-bd0af21f182b@onelake.dfs.fabric.microsoft.com/7c6d771a-3b6f-4042-8a89-1a885973a93c/Files/templates/emails/output",
        output_path="abfss://ab08da5e-0f71-423b-a811-bd0af21f182b@onelake.dfs.fabric.microsoft.com/7c6d771a-3b6f-4042-8a89-1a885973a93c/decrypted",
        pgp_enabled=pgp_enabled,
        key_vault_name=key_vault_name,
        public_key_secret=public_key_secret,
        passphrase=passphrase
    )
except Exception as e:
    print(f"Error during decryption: {str(e)}")

________________________________
Create a reusable and environment-aware notebook that automates the initialization and seeding of tables required for the Common Policy Data Product. The notebook should:

Create required tables if they do not exist, using predefined schemas.
Load default or seed data into the tables from version-controlled sources.
Be idempotentâ€”safe to run multiple times without causing duplication or data corruption.
Include logging, error handling, and validation steps to ensure successful execution.
Support configuration for different environments (e.g., dev, prod) via parameters or environment variables.
This notebook will be used during deployment or setup to ensure consistent and reliable data product initialization across environments.
______________________________________
CREATE TABLE IF NOT EXISTS `dim_lob` (
  `lob_key` INT,
  `lob_cd_bus_key` STRING,
  `lob_desc` STRING,
  `dl_eltid` STRING,
  `dl_runid` STRING,
  `dl_row_insert_agent` STRING,
  `dl_row_hash` STRING,
  `dl_is_current_flag` TINYINT,
  `dl_row_expiration_date` DATE,
  `dl_row_effective_date` DATE,
  `dl_row_update_timestamp` TIMESTAMP,
  `dl_row_insert_timestamp` TIMESTAMP
) USING DELTA COMMENT "A category of insurance that groups similar types of policies together based on the nature of risk covered. For example, Homeowners Insurance is a line of business for residential property coverage.";


CREATE TABLE IF NOT EXISTS `dim_carrier` (
  `carrier_key` INT,
  `carrier_type` STRING,
  `carrier_nm` STRING,
  `carrier_cd_bus_key` STRING,
  `dl_eltid` STRING,
  `dl_runid` STRING,
  `dl_row_insert_agent` STRING,
  `dl_row_hash` STRING,
  `dl_is_current_flag` TINYINT,
  `dl_row_expiration_date` DATE,
  `dl_row_effective_date` DATE,
  `dl_row_update_timestamp` TIMESTAMP,
  `dl_row_insert_timestamp` TIMESTAMP
) USING DELTA COMMENT "An insurance company that provides coverage and assumes risk. A carrier can be classified as Admitted OR non-admitted.";

CREATE TABLE IF NOT EXISTS `dim_insured_legal_entity_type` (
  `insd_leg_ent_type_key` INT,
  `insd_leg_ent_type_cd_bus_key` STRING,
  `insd_leg_ent_type_desc` STRING,
  `dl_eltid` STRING,
  `dl_runid` STRING,
  `dl_row_insert_agent` STRING,
  `dl_row_hash` STRING,
  `dl_is_current_flag` TINYINT,
  `dl_row_expiration_date` DATE,
  `dl_row_effective_date` DATE,
  `dl_row_update_timestamp` TIMESTAMP,
  `dl_row_insert_timestamp` TIMESTAMP
) USING DELTA;

# Import necessary modules
from pyspark.sql import SparkSession
import logging

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Assume the input file is uploaded to the lakehouse Files section
# Adjust the path if needed, e.g., abfss://<workspace>@<storage>.dfs.core.windows.net/Files/input_file.txt
file_path = "Files/input_file.txt"  # Relative path in Fabric lakehouse

# Step 1: Read the input file
try:
    logger.info(f"Reading file from {file_path}")
    ddl_content = spark.read.text(file_path).collect()
    ddl_text = "\n".join([row.value for row in ddl_content])
    logger.info("File read successfully")
except Exception as e:
    logger.error(f"Error reading file: {str(e)}")
    raise

# Step 2: Parse the DDL statements (assuming statements are separated by ';')
ddl_statements = [stmt.strip() + ';' for stmt in ddl_text.split(';') if stmt.strip()]

# Step 3: Execute each DDL statement with error handling
for i, stmt in enumerate(ddl_statements):
    try:
        logger.info(f"Executing DDL statement {i+1}: {stmt[:100]}...")  # Log first 100 chars
        spark.sql(stmt)
        logger.info(f"DDL statement {i+1} executed successfully")
    except Exception as e:
        logger.error(f"Error executing DDL statement {i+1}: {str(e)}")
        # Continue or raise based on requirement; here we continue to try all

# Step 4: Validation - Check if tables exist
tables_to_validate = ['dim_lob', 'dim_carrier', 'dim_insured_legal_entity_type']  # Extracted from DDL

for table in tables_to_validate:
    try:
        logger.info(f"Validating table: {table}")
        spark.sql(f"DESCRIBE {table}").show()
        logger.info(f"Table {table} exists and is valid")
    except Exception as e:
        logger.error(f"Validation failed for table {table}: {str(e)}")

logger.info("Notebook execution completed")
=====================================================
# Import necessary modules
from pyspark.sql import SparkSession
import logging
import re

# Configuration
lakehouse_name = "my_lakehouse"
schema_name = "test"

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Assume the input file is uploaded to the lakehouse Files section
file_path = f"abfss://{lakehouse_name}@onelake.dfs.fabric.microsoft.com/Files/input_file.txt"

# Step 1: Read the input file
try:
    logger.info(f"Reading file from {file_path}")
    ddl_content = spark.read.text(file_path).collect()
    ddl_text = "\n".join([row.value for row in ddl_content])
    logger.info("File read successfully")
except Exception as e:
    logger.error(f"Error reading file: {str(e)}")
    raise

# Step 2: Extract table names from DDL
try:
    logger.info("Extracting table names from DDL")
    # Regular expression to match CREATE TABLE statements and extract table names
    table_pattern = r"CREATE TABLE IF NOT EXISTS\s+[`']?(\w+)[`']?\s*\("
    table_names = re.findall(table_pattern, ddl_text, re.IGNORECASE)
    if not table_names:
        logger.error("No table names found in DDL")
        raise ValueError("No valid CREATE TABLE statements found in DDL")
    logger.info(f"Extracted table names: {table_names}")
except Exception as e:
    logger.error(f"Error extracting table names: {str(e)}")
    raise

# Step 3: Ensure schema exists
try:
    logger.info(f"Ensuring schema {schema_name} exists")
    spark.sql(f"CREATE SCHEMA IF NOT EXISTS {schema_name}")
    logger.info(f"Schema {schema_name} ready")
    # Set the schema as the current context
    spark.sql(f"USE {schema_name}")
    logger.info(f"Schema {schema_name} set as current context")
except Exception as e:
    logger.error(f"Error creating or setting schema {schema_name}: {str(e)}")
    raise

# Step 4: Parse the DDL statements (assuming statements are separated by ';')
ddl_statements = [stmt.strip() + ';' for stmt in ddl_text.split(';') if stmt.strip()]

# Step 5: Execute each DDL statement in the context of the schema
for i, stmt in enumerate(ddl_statements):
    try:
        # Replace CREATE TABLE with schema-qualified table name
        modified_stmt = stmt.replace("CREATE TABLE IF NOT EXISTS `", f"CREATE TABLE IF NOT EXISTS `{schema_name}.")
        logger.info(f"Executing DDL statement {i+1}: {modified_stmt[:100]}...")
        spark.sql(modified_stmt)
        logger.info(f"DDL statement {i+1} executed successfully")
    except Exception as e:
        logger.error(f"Error executing DDL statement {i+1}: {str(e)}")
        continue

# Step 6: Validation - Check if extracted tables exist in the specified schema
for table in table_names:
    try:
        logger.info(f"Validating table: {schema_name}.{table}")
        spark.sql(f"DESCRIBE {schema_name}.{table}").show()
        logger.info(f"Table {schema_name}.{table} exists and is valid")
    except Exception as e:
        logger.error(f"Validation failed for table {schema_name}.{table}: {str(e)}")

logger.info("Notebook execution completed")
